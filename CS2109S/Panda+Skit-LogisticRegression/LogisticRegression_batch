def logistic_regression_batch_gradient_descent(X_train: np.ndarray, y_train: np.ndarray, max_num_epochs: int = 250, threshold: np.float64 = 0.05, alpha: np.float64 = 1e-5):
    '''
    Initialize your weight to zeros. Write your terminating condition, and run the weight update for some iterations.
    Get the resulting weight vector. Use that to do predictions, and output the final weight vector.
    Parameters
    ----------
    X_train: np.ndarray
        (m, n) training dataset (features).
    y_train: np.ndarray
        (m,) training dataset (corresponding targets).
    max_num_epochs: int
        this should be one of the terminating conditions. 
        That means if you initialize num_update_rounds to 0, 
        then you should stop updating the weights when num_update_rounds >= max_num_epochs.
    threshold: np.float64
        terminating when error <= threshold value, or if you reach the max number of update rounds first.
    alpha: np.float64
        logistic regression learning rate.
    Returns
    -------
    The final (n,) weight parameters
    '''
    # TODO: add your solution here and remove `raise NotImplementedError`
    num_update_rounds = 0
    m = X_train.shape[0]
    n = X_train.shape[1]
    weights = np.zeros(n)
    error = cost_function(X_train, y_train, weights)
    
    
    while ((error > threshold) and (num_update_rounds < max_num_epochs)):
        weights = weight_update(X_train, y_train, alpha, weights)
        num_update_rounds += 1
        error = cost_function(X_train, y_train, weights)
    
    return weights
