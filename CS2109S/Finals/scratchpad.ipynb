{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d017333",
   "metadata": {},
   "source": [
    "# Final Assessment Scratch Pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d00386",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea516aa7",
   "metadata": {},
   "source": [
    "1. Please use only this Jupyter notebook to work on your model, and **do not use any extra files**. If you need to define helper classes or functions, feel free to do so in this notebook.\n",
    "2. This template is intended to be general, but it may not cover every use case. The sections are given so that it will be easier for us to grade your submission. If your specific use case isn't addressed, **you may add new Markdown or code blocks to this notebook**. However, please **don't delete any existing blocks**.\n",
    "3. If you don't think a particular section of this template is necessary for your work, **you may skip it**. Be sure to explain clearly why you decided to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022cb4cd",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "17448887-70bf-4445-84f4-73f75a59ad77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nOverview\\nI used a convolution neural network\\n\\n1. Descriptive Analysis\\nFrom looking at the labels, I ascertained that \\n- There was an overwhelming majority of 0 classes, with few 1s and even less 2s. Meaning class imbalance will be an issue to be addressed\\n- There were the prescence of NaN results.\\nFrom the image files, I found that\\n- Values tended to hover around 0,255, with the presence of massive outliers\\n- There were the presence of NaN results\\n\\n2. Detection and Handling of Missing Values\\nI decided to drop all images with NaN result rows, which accounted for ~10% of the data, which while not negligable is not\\nsizable enough to warrant the computational cost of predicting values, and filling with the mode when the data is already imbalanced is impractical.\\n\\nFor NaN values in the image batch, I aim to replace nan values with 0, as a nan value represents a third of a pixel in a 16x16 image, \\nnot sufficient enough of a percentage of the image to warrant dropping the entire image.\\n\\n3. Detection and Handling of Outliers\\nOutliers were certainly present (known from descriptive analysis, min and max << or >> mean, or even 1st/3rd quartile values\\n\\n4. Detection and Handling of Class Imbalance\\nBased on the descriptive analysis stage, it was very clear that the classes were extremely unbalanced. In order to handle this I considered several approaches\\n    Oversampling: The method I ended up using, tradeoff that it is not able to capture the patterns in 2 and 1 particularly well as they were duplicated\\n                  many times over\\n    Undersampling: As the test data mirrors the distribution of the training data, I decided that sacrificing the ability to capture the majority class's\\n                   patterns would be far more detrimental to accuracy than oversampling, although it would address the class imbalance problem\\n    SMOTE: Without the library included, I felt this was not feasable to implement\\n                   \\n\\n5. Understanding Relationship Between Variables\\nDue to my CNN approach, Finding and modifying/removing corelated data was not necessary/possible due to the spatial-exploiting nature of CNNs.\\n6. Data Visualization\\n[TODO]\\n\\n7. General Preprocessing\\nImage Processing:\\n    Use np.nan_to_num() to set all nan values to 0\\n    Use np.clip() to clip values strictly to range 0,255. The reasoning behind 0, 255 is that it is the standard values for image pixel rgb values, while\\n    the data may not necessarily be in rgb, The data hovered around that range, and clipping to 0, 255 could help in data visualization as it can be \\n    converted to images.\\n    I did not perform normalization using z-score or interquartile range as my model uses batch normalization layers that help perform that for me\\n\\n8. Feature Selection + 9. Feature Engineering\\nInitially, I considered PCA dimensionality reduction followed by neural network, but as the data input is an image I decided to choose CNN.\\nAs I opted for a CNN architecture that is designed to exploit the spatial features of an image, PCA/dimensionality reduction is not beneficial to my model.\\nIn a sense, CNN performs feature selection+engineering by detecting patterns during the training loop.\\n\\n10. Creating Models\\nMODEL 1: CNN with Adam, dropout, clipping\\nMy initial attempt used a basic Convolution Neural Network using the nn.module() class. My initial design followed closely to the that of PS7 that being\\nconv2d -> maxpool -> ReLU -> ... flatten -> Linear -> Linear -> softmax -> 3 classes to obtain probabilities\\nAs i inherited from nn.module, i implemented forward by piecing together all the layers together in correct sequence\\nfor fit(X, y), I followed the following procedure:\\n    Preprocess X and y\\n    Convet X and y into tensors\\n    using an optimizer (Adam) , train the model for a fixed amount of epochs\\n    return the model\\n\\nAs for predict, I did the following:\\n    run forward onto the input\\n    obtain a N row 2d matrice with 3 values each s.t. higher value implies higher probability that the value is the class [index]\\n    use torch.argmax to obtain the index of the max probability, obtaining my predictions\\nImprovements\\nChanged leakyReLU to ReLU as values are clipped to >0, so leaky doesnt provide much functionality + is much slower\\nPreprocess data in input -> F1 scores of ~0.30-0.34\\nIntroduced clipping to preprocessing image stage for both prediction and fit inputs (drastically improved performance), introduced dropout layers in the \\nCNN, prevents overfitting s.t. it performs better on the X_test -> F1 scores of ~0.5-0.6\\n\\nMODEL Variations\\nPCA -> MLP Classifier: Recquires a lot more preprocessing as non-convnet nns dont utilize spatialinformation, pca results in loss in information regarding \\ndimensions as well, leading to very inaccurate, not very fast results, CNN much better in terms of F1 score AND speed.\\nRandom transformations: Considered adding random transformations to the images during training to introduce variance, but mixture of runtime increasing\\n11. Model Evaluation\\nBelow are some alternatives I considered regarding several features of the model\\nOptimizer: Between Adam and SGD, I found that SGD did not converge as well even when run for more epochs and time, so I opted for Adam.\\n\\n\\n12. Hyperparameters Search\\nI used (non-exchaustive) grid search, iteratively looping through learning rates, dropout rates, and layers in linear layer for 3-5 values each in order\\nto determine what specific values were good. While there is a probabilistic element (model returns 0.72 then 0.42 the next run without changing anything)\\nlikely due to the random element in dropout layers, I found ~good values of lr = 0.001, dropout = 0.3, layers = 1024 that tended to yield the best result\\n\\nIn order to carry out the grid search, I made tables of different values and looped through each combination of each hyperparam value, building a model \\nthose hyperparams and returning F1 score.\\n\\nConclusion\\nThe most important/helpful thing was inferring that because the data is in the form of images, CNNs that exploit the spatial nature of images would be\\ngood as it handles many aspects such as feature engineering+selection during the training process.\\n\\n\""
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Overview\n",
    "I used a convolution neural network\n",
    "\n",
    "1. Descriptive Analysis\n",
    "From looking at the labels converting to a pd.Dataframe and using .describe(), I ascertained that \n",
    "- There was an overwhelming majority of 0 classes, with few 1s and even less 2s. Meaning class imbalance will be an issue to be addressed\n",
    "- There were the prescence of NaN results.\n",
    "From the image files, I found that\n",
    "- Values tended to hover around 0,255, with the presence of massive outliers\n",
    "- There were the presence of NaN results\n",
    "\n",
    "2. Detection and Handling of Missing Values\n",
    "I decided to drop all images with NaN result rows, which accounted for ~10% of the data, which while not negligable is not\n",
    "sizable enough to warrant the computational cost of predicting values, and filling with the mode when the data is already imbalanced is impractical.\n",
    "\n",
    "For NaN values in the image batch, I aim to replace nan values with 0, as a nan value represents a third of a pixel in a 16x16 image, \n",
    "not sufficient enough of a percentage of the image to warrant dropping the entire image.\n",
    "\n",
    "3. Detection and Handling of Outliers\n",
    "Outliers were certainly present (known from descriptive analysis, min and max << or >> mean, or even 1st/3rd quartile values\n",
    "\n",
    "4. Detection and Handling of Class Imbalance\n",
    "Based on the descriptive analysis stage, it was very clear that the classes were extremely unbalanced. In order to handle this I considered several approaches\n",
    "    Oversampling: The method I ended up using, tradeoff that it is not able to capture the patterns in 2 and 1 particularly well as they were duplicated\n",
    "                  many times over\n",
    "    Undersampling: As the test data mirrors the distribution of the training data, I decided that sacrificing the ability to capture the majority class's\n",
    "                   patterns would be far more detrimental to accuracy than oversampling, although it would address the class imbalance problem\n",
    "                   \n",
    "5. Understanding Relationship Between Variables\n",
    "Due to my CNN approach, Finding and modifying/removing corelated data was not necessary/possible due to the spatial-exploiting nature of CNNs.\n",
    "\n",
    "6. Data visualization[CELL BELOW]\n",
    "As i clipped the values to 0, 255 I plotted the first 4 values (that represented a 0, 1, 2, and NaN) using mtplot, but nothing of note can be observed\n",
    "\n",
    "7. General Preprocessing\n",
    "Image Processing:\n",
    "    set all nan values to 0\n",
    "    clip values strictly to range 0,255. The reasoning behind 0, 255 is that it is the standard values for image pixel rgb values, while\n",
    "    the data may not necessarily be in rgb, The data hovered around that range, and clipping to 0, 255 could help in data visualization as it can be \n",
    "    converted to images.\n",
    "    did not perform normalization using z-score or interquartile range as my model uses batch normalization layers that help perform that for me\n",
    "\n",
    "8. Feature Selection + 9. Feature Engineering\n",
    "Initially, I considered PCA dimensionality reduction followed by neural network, but as the data input is an image I decided to choose CNN.\n",
    "As I opted for a CNN architecture that is designed to exploit the spatial features of an image, PCA/dimensionality reduction is not beneficial to my model.\n",
    "In a sense, CNN performs feature selection+engineering by detecting patterns during the training loop.\n",
    "\n",
    "10. Creating Models + 11.Evalutation\n",
    "MODEL 1: CNN with Adam, dropout, clipping\n",
    "My initial attempt used a basic Convolution Neural Network using the nn.module() class. My initial design followed closely to the that of PS7 that being\n",
    "conv2d -> maxpool -> ReLU -> ... flatten -> Linear -> Linear -> softmax -> 3 classes using dropout along the way to obtain probabilities\n",
    "As i inherited from nn.module, i implemented forward by piecing together all the layers together in correct sequence\n",
    "for fit(X, y), I followed the following procedure:\n",
    "    Preprocess X and y\n",
    "    Convet X and y into tensors\n",
    "    using an optimizer (Adam) , train the model for a fixed amount of epochs\n",
    "    return the model\n",
    "\n",
    "As for predict, I did the following:\n",
    "    run forward onto the input\n",
    "    obtain a N row 2d matrice with 3 values each s.t. higher value implies higher probability that the value is the class [index]\n",
    "    use torch.argmax to obtain the index of the max probability, obtaining my predictions\n",
    "\n",
    "UNUSED MODEL + Variations\n",
    "PCA -> MLP Classifier: Recquires a lot more preprocessing as non-convnet nns dont utilize spatialinformation, pca results in loss in information regarding \n",
    "dimensions as well, leading to very inaccurate results CNN much better in terms of F1 score despite taking a bit more time\n",
    "*unused PCA model is under workings -> step 11\n",
    "\n",
    "Random transformations: Considered adding random transformations to the images during training to introduce variance, but mixture of runtime increasing\n",
    "drastically as well as loss in accuracy led to me not using it \n",
    "\n",
    "\n",
    "12. Hyperparameters Search\n",
    "I used (non-exchaustive) grid search, iteratively looping through learning rates, dropout rates, and layers in linear layer for 3-5 values each in order\n",
    "to determine what specific values were good. While there is a probabilistic element (model returns 0.72 then 0.42 the next run without changing anything)\n",
    "likely due to the random element in dropout layers, I found ~good values of lr = 0.001, dropout = 0.3, layers = 1024 that tended to yield the best result\n",
    "*Code for grid search is under working-> q12\n",
    "In order to carry out the grid search, I made tables of different values and looped through each combination of each hyperparam value, building a model \n",
    "those hyperparams and returning F1 score.\n",
    "\n",
    "Conclusion\n",
    "The most important/helpful thing was inferring that because the data is in the form of images, CNNs that exploit the spatial nature of images would be\n",
    "good as it handles many aspects such as feature engineering+selection during the training process.\n",
    "\n",
    "Chat-GPT conversation\n",
    "https://chat.openai.com/share/64bab8df-7c17-4829-813e-c522043d4614\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3732a-8d78-454c-a32a-b948dbbb7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "#descriptive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "cb9859c7-532d-4676-b5b8-7c52db13c5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#image visualisation\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = np.transpose(images[i], (1, 2, 0)) / 255\n",
    "    img = img.astype('float32')\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')  # Turn off axis numbers\n",
    "    ax.set_title(labels[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcaf29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27103374",
   "metadata": {},
   "source": [
    "# Workings (Not Graded)\n",
    "\n",
    "You will do your working below. Note that anything below this section will not be graded, but we might counter-check what you wrote in the report above with your workings to make sure that you actually did what you claimed to have done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c6cd4",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "\n",
    "Here, we import some packages necessary to run this notebook. In addition, you may import other packages as well. Do note that when submitting your model, you may only use packages that are available in Coursemology (see `main.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "cded1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748c35d7",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "The dataset `data/images.npy` is of size $(N, C, H, W)$, where $N$, $C$, $H$, and $W$ correspond to the number of data, image channels, image width, and image height, respectively.\n",
    "\n",
    "A code snippet that loads the data is provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09da291",
   "metadata": {},
   "source": [
    "### Load Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "6297e25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2911, 3, 16, 16)\n",
      "Labels: (2911,)\n"
     ]
    }
   ],
   "source": [
    "with open('data.npy', 'rb') as f:\n",
    "    data = np.load(f, allow_pickle=True).item()\n",
    "    images = data['image']\n",
    "    labels = data['label']\n",
    "    \n",
    "print('Shape:', images.shape)\n",
    "print('Labels:', labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe832b6",
   "metadata": {},
   "source": [
    "## Data Exploration & Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a464c",
   "metadata": {},
   "source": [
    "### 1. Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "3b1f62dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  0.09656\n",
      "std:  0.3262\n",
      "0.0    2392\n",
      "1.0     203\n",
      "2.0      25\n",
      "dtype: int64\n",
      "nan results:  291\n",
      "(2911, 768)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>60.406250</td>\n",
       "      <td>77.43750</td>\n",
       "      <td>91.68750</td>\n",
       "      <td>58.843750</td>\n",
       "      <td>28.15625</td>\n",
       "      <td>37.15625</td>\n",
       "      <td>24.750000</td>\n",
       "      <td>35.281250</td>\n",
       "      <td>54.187500</td>\n",
       "      <td>89.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>253.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>43.031250</td>\n",
       "      <td>57.00000</td>\n",
       "      <td>61.62500</td>\n",
       "      <td>23.515625</td>\n",
       "      <td>27.84375</td>\n",
       "      <td>20.09375</td>\n",
       "      <td>18.546875</td>\n",
       "      <td>21.656250</td>\n",
       "      <td>90.187500</td>\n",
       "      <td>50.15625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>237.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.031250</td>\n",
       "      <td>40.25000</td>\n",
       "      <td>36.84375</td>\n",
       "      <td>66.937500</td>\n",
       "      <td>82.75000</td>\n",
       "      <td>24.43750</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>27.546875</td>\n",
       "      <td>78.687500</td>\n",
       "      <td>33.43750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>237.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>7456.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>...</td>\n",
       "      <td>47.375000</td>\n",
       "      <td>52.03125</td>\n",
       "      <td>92.31250</td>\n",
       "      <td>23.812500</td>\n",
       "      <td>72.81250</td>\n",
       "      <td>44.28125</td>\n",
       "      <td>55.750000</td>\n",
       "      <td>54.187500</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>83.31250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78.0</td>\n",
       "      <td>-3164.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>...</td>\n",
       "      <td>94.812500</td>\n",
       "      <td>59.78125</td>\n",
       "      <td>35.90625</td>\n",
       "      <td>26.921875</td>\n",
       "      <td>64.75000</td>\n",
       "      <td>76.50000</td>\n",
       "      <td>68.437500</td>\n",
       "      <td>46.125000</td>\n",
       "      <td>77.125000</td>\n",
       "      <td>61.93750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2906</th>\n",
       "      <td>145.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>6812.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83.312500</td>\n",
       "      <td>24.75000</td>\n",
       "      <td>93.87500</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>42.12500</td>\n",
       "      <td>77.75000</td>\n",
       "      <td>20.406250</td>\n",
       "      <td>53.593750</td>\n",
       "      <td>51.406250</td>\n",
       "      <td>65.68750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2907</th>\n",
       "      <td>122.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26.296875</td>\n",
       "      <td>51.71875</td>\n",
       "      <td>50.78125</td>\n",
       "      <td>75.250000</td>\n",
       "      <td>62.56250</td>\n",
       "      <td>73.75000</td>\n",
       "      <td>80.250000</td>\n",
       "      <td>39.937500</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>51.09375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2908</th>\n",
       "      <td>44.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5088.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>...</td>\n",
       "      <td>86.437500</td>\n",
       "      <td>96.06250</td>\n",
       "      <td>76.50000</td>\n",
       "      <td>67.812500</td>\n",
       "      <td>34.37500</td>\n",
       "      <td>37.46875</td>\n",
       "      <td>66.937500</td>\n",
       "      <td>23.515625</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2909</th>\n",
       "      <td>129.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>56.375000</td>\n",
       "      <td>47.68750</td>\n",
       "      <td>31.56250</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>48.93750</td>\n",
       "      <td>23.81250</td>\n",
       "      <td>31.562500</td>\n",
       "      <td>75.562500</td>\n",
       "      <td>30.640625</td>\n",
       "      <td>32.81250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2910</th>\n",
       "      <td>64.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.093750</td>\n",
       "      <td>47.68750</td>\n",
       "      <td>60.40625</td>\n",
       "      <td>42.718750</td>\n",
       "      <td>49.25000</td>\n",
       "      <td>59.78125</td>\n",
       "      <td>69.062500</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>62.250000</td>\n",
       "      <td>87.37500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2911 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1      2      3       4       5      6      7       8    \\\n",
       "0      36.0     NaN   20.0   88.0   145.0    52.0   14.0  128.0   144.0   \n",
       "1     253.0   240.0  205.0   99.0     6.0    66.0   93.0    0.0    27.0   \n",
       "2     237.0   148.0    3.0   78.0   213.0   251.0  240.0  176.0   158.0   \n",
       "3     237.0   208.0  151.0   88.0    46.0  7456.0  133.0  207.0   241.0   \n",
       "4      78.0 -3164.0  106.0  122.0   123.0    72.0   80.0  213.0   247.0   \n",
       "...     ...     ...    ...    ...     ...     ...    ...    ...     ...   \n",
       "2906  145.0   178.0  225.0  151.0    63.0    99.0  142.0  149.0  6812.0   \n",
       "2907  122.0   119.0  121.0  163.0   210.0   225.0  211.0  157.0   130.0   \n",
       "2908   44.0    38.0   27.0    4.0 -5088.0    23.0   60.0   87.0   150.0   \n",
       "2909  129.0   161.0  217.0  226.0   198.0   112.0   51.0   57.0    45.0   \n",
       "2910   64.0   110.0  183.0  131.0    41.0     3.0    0.0   12.0    61.0   \n",
       "\n",
       "        9    ...        758       759       760        761       762  \\\n",
       "0      60.0  ...  60.406250  77.43750  91.68750  58.843750  28.15625   \n",
       "1       NaN  ...  43.031250  57.00000  61.62500  23.515625  27.84375   \n",
       "2     188.0  ...  61.031250  40.25000  36.84375  66.937500  82.75000   \n",
       "3     233.0  ...  47.375000  52.03125  92.31250  23.812500  72.81250   \n",
       "4     176.0  ...  94.812500  59.78125  35.90625  26.921875  64.75000   \n",
       "...     ...  ...        ...       ...       ...        ...       ...   \n",
       "2906  215.0  ...  83.312500  24.75000  93.87500  88.000000  42.12500   \n",
       "2907  131.0  ...  26.296875  51.71875  50.78125  75.250000  62.56250   \n",
       "2908  241.0  ...  86.437500  96.06250  76.50000  67.812500  34.37500   \n",
       "2909   17.0  ...  56.375000  47.68750  31.56250  30.015625  48.93750   \n",
       "2910  133.0  ...  29.093750  47.68750  60.40625  42.718750  49.25000   \n",
       "\n",
       "           763        764        765        766       767  \n",
       "0     37.15625  24.750000  35.281250  54.187500  89.25000  \n",
       "1     20.09375  18.546875  21.656250  90.187500  50.15625  \n",
       "2     24.43750  57.000000  27.546875  78.687500  33.43750  \n",
       "3     44.28125  55.750000  54.187500  30.328125  83.31250  \n",
       "4     76.50000  68.437500  46.125000  77.125000  61.93750  \n",
       "...        ...        ...        ...        ...       ...  \n",
       "2906  77.75000  20.406250  53.593750  51.406250  65.68750  \n",
       "2907  73.75000  80.250000  39.937500  66.000000  51.09375  \n",
       "2908  37.46875  66.937500  23.515625  64.750000  64.75000  \n",
       "2909  23.81250  31.562500  75.562500  30.640625  32.81250  \n",
       "2910  59.78125  69.062500  72.500000  62.250000  87.37500  \n",
       "\n",
       "[2911 rows x 768 columns]"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(labels)\n",
    "mean = df.mean()\n",
    "std = df.std()\n",
    "print('mean: ', mean[0])\n",
    "print('std: ', std[0])\n",
    "print(df.value_counts())\n",
    "print('nan results: ', df.isna().sum().sum())\n",
    "\n",
    "n, c, h, w = images.shape\n",
    "\n",
    "#flatten all channels to make pandas dataframe of images\n",
    "images_flatten = images.reshape(n, c, -1)\n",
    "reshaped_flattened = images_flatten.reshape(images_flatten.shape[0], -1)\n",
    "print(reshaped_flattened.shape)\n",
    "df_img = pd.DataFrame(reshaped_flattened)\n",
    "df_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb61967",
   "metadata": {},
   "source": [
    "### 2. Detection and Handling of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "4bb9cdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2620,)\n",
      "(2620, 3, 256)\n"
     ]
    }
   ],
   "source": [
    "#remove rows where result is nan using a mask\n",
    "nan = np.isnan(labels)\n",
    "nan_indexes = np.argwhere(np.isnan(labels)).squeeze()\n",
    "mask = np.ones(labels.shape, bool)\n",
    "mask[nan_indexes] = False\n",
    "\n",
    "labels = labels[mask]\n",
    "images_flatten = images_flatten[mask]\n",
    "print(labels.shape)\n",
    "print(images_flatten.shape)\n",
    "#update n \n",
    "n  = images_flatten.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "6903cf49-99d0-48fe-9502-0bb203459538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  0.09656\n",
      "std:  0.3262\n",
      "0.0    2392\n",
      "1.0     203\n",
      "2.0      25\n",
      "dtype: int64\n",
      "nan results:  0\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(labels)\n",
    "mean = df.mean()\n",
    "std = df.std()\n",
    "print('mean: ', mean[0])\n",
    "print('std: ', std[0])\n",
    "print(df.value_counts())\n",
    "print('nan results: ', df.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "a532bf7b-2759-49f2-a217-c344e1e63491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill nan values in image data with 0\n",
    "images_flatten = np.nan_to_num(images_flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adcb9cd",
   "metadata": {},
   "source": [
    "### 3. Detection and Handling of Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "816baa2b-b182-49f7-99c8-97e5e965f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove nans and clip values\n",
    "images = np.clip(np.nan_to_num(images), 0, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "ed1c17a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2620, 768)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>253.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>43.031250</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>61.62500</td>\n",
       "      <td>23.515625</td>\n",
       "      <td>27.843750</td>\n",
       "      <td>20.09375</td>\n",
       "      <td>18.546875</td>\n",
       "      <td>21.656250</td>\n",
       "      <td>90.187500</td>\n",
       "      <td>50.15625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>237.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.031250</td>\n",
       "      <td>40.250000</td>\n",
       "      <td>36.84375</td>\n",
       "      <td>66.937500</td>\n",
       "      <td>82.750000</td>\n",
       "      <td>24.43750</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>27.546875</td>\n",
       "      <td>78.687500</td>\n",
       "      <td>33.43750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>237.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>7456.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>...</td>\n",
       "      <td>47.375000</td>\n",
       "      <td>52.031250</td>\n",
       "      <td>92.31250</td>\n",
       "      <td>23.812500</td>\n",
       "      <td>72.812500</td>\n",
       "      <td>44.28125</td>\n",
       "      <td>55.750000</td>\n",
       "      <td>54.187500</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>83.31250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78.0</td>\n",
       "      <td>-3164.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>...</td>\n",
       "      <td>94.812500</td>\n",
       "      <td>59.781250</td>\n",
       "      <td>35.90625</td>\n",
       "      <td>26.921875</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>76.50000</td>\n",
       "      <td>68.437500</td>\n",
       "      <td>46.125000</td>\n",
       "      <td>77.125000</td>\n",
       "      <td>61.93750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83.937500</td>\n",
       "      <td>19.796875</td>\n",
       "      <td>63.18750</td>\n",
       "      <td>86.125000</td>\n",
       "      <td>29.703125</td>\n",
       "      <td>48.62500</td>\n",
       "      <td>49.875000</td>\n",
       "      <td>65.375000</td>\n",
       "      <td>95.750000</td>\n",
       "      <td>34.37500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2615</th>\n",
       "      <td>145.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>6812.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83.312500</td>\n",
       "      <td>24.750000</td>\n",
       "      <td>93.87500</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>42.125000</td>\n",
       "      <td>77.75000</td>\n",
       "      <td>20.406250</td>\n",
       "      <td>53.593750</td>\n",
       "      <td>51.406250</td>\n",
       "      <td>65.68750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2616</th>\n",
       "      <td>122.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26.296875</td>\n",
       "      <td>51.718750</td>\n",
       "      <td>50.78125</td>\n",
       "      <td>75.250000</td>\n",
       "      <td>62.562500</td>\n",
       "      <td>73.75000</td>\n",
       "      <td>80.250000</td>\n",
       "      <td>39.937500</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>51.09375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2617</th>\n",
       "      <td>44.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5088.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>...</td>\n",
       "      <td>86.437500</td>\n",
       "      <td>96.062500</td>\n",
       "      <td>76.50000</td>\n",
       "      <td>67.812500</td>\n",
       "      <td>34.375000</td>\n",
       "      <td>37.46875</td>\n",
       "      <td>66.937500</td>\n",
       "      <td>23.515625</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2618</th>\n",
       "      <td>129.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>56.375000</td>\n",
       "      <td>47.687500</td>\n",
       "      <td>31.56250</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>48.937500</td>\n",
       "      <td>23.81250</td>\n",
       "      <td>31.562500</td>\n",
       "      <td>75.562500</td>\n",
       "      <td>30.640625</td>\n",
       "      <td>32.81250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2619</th>\n",
       "      <td>64.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.093750</td>\n",
       "      <td>47.687500</td>\n",
       "      <td>60.40625</td>\n",
       "      <td>42.718750</td>\n",
       "      <td>49.250000</td>\n",
       "      <td>59.78125</td>\n",
       "      <td>69.062500</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>62.250000</td>\n",
       "      <td>87.37500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2620 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1      2      3       4       5      6      7       8    \\\n",
       "0     253.0   240.0  205.0   99.0     6.0    66.0   93.0    0.0    27.0   \n",
       "1     237.0   148.0    3.0   78.0   213.0   251.0  240.0  176.0   158.0   \n",
       "2     237.0   208.0  151.0   88.0    46.0  7456.0  133.0  207.0   241.0   \n",
       "3      78.0 -3164.0  106.0  122.0   123.0    72.0   80.0  213.0   247.0   \n",
       "4       0.0    23.0  101.0  152.0   174.0   137.0   91.0   52.0    54.0   \n",
       "...     ...     ...    ...    ...     ...     ...    ...    ...     ...   \n",
       "2615  145.0   178.0  225.0  151.0    63.0    99.0  142.0  149.0  6812.0   \n",
       "2616  122.0   119.0  121.0  163.0   210.0   225.0  211.0  157.0   130.0   \n",
       "2617   44.0    38.0   27.0    4.0 -5088.0    23.0   60.0   87.0   150.0   \n",
       "2618  129.0   161.0  217.0  226.0   198.0   112.0   51.0   57.0    45.0   \n",
       "2619   64.0   110.0  183.0  131.0    41.0     3.0    0.0   12.0    61.0   \n",
       "\n",
       "        9    ...        758        759       760        761        762  \\\n",
       "0       0.0  ...  43.031250  57.000000  61.62500  23.515625  27.843750   \n",
       "1     188.0  ...  61.031250  40.250000  36.84375  66.937500  82.750000   \n",
       "2     233.0  ...  47.375000  52.031250  92.31250  23.812500  72.812500   \n",
       "3     176.0  ...  94.812500  59.781250  35.90625  26.921875  64.750000   \n",
       "4      95.0  ...  83.937500  19.796875  63.18750  86.125000  29.703125   \n",
       "...     ...  ...        ...        ...       ...        ...        ...   \n",
       "2615  215.0  ...  83.312500  24.750000  93.87500  88.000000  42.125000   \n",
       "2616  131.0  ...  26.296875  51.718750  50.78125  75.250000  62.562500   \n",
       "2617  241.0  ...  86.437500  96.062500  76.50000  67.812500  34.375000   \n",
       "2618   17.0  ...  56.375000  47.687500  31.56250  30.015625  48.937500   \n",
       "2619  133.0  ...  29.093750  47.687500  60.40625  42.718750  49.250000   \n",
       "\n",
       "           763        764        765        766       767  \n",
       "0     20.09375  18.546875  21.656250  90.187500  50.15625  \n",
       "1     24.43750  57.000000  27.546875  78.687500  33.43750  \n",
       "2     44.28125  55.750000  54.187500  30.328125  83.31250  \n",
       "3     76.50000  68.437500  46.125000  77.125000  61.93750  \n",
       "4     48.62500  49.875000  65.375000  95.750000  34.37500  \n",
       "...        ...        ...        ...        ...       ...  \n",
       "2615  77.75000  20.406250  53.593750  51.406250  65.68750  \n",
       "2616  73.75000  80.250000  39.937500  66.000000  51.09375  \n",
       "2617  37.46875  66.937500  23.515625  64.750000  64.75000  \n",
       "2618  23.81250  31.562500  75.562500  30.640625  32.81250  \n",
       "2619  59.78125  69.062500  72.500000  62.250000  87.37500  \n",
       "\n",
       "[2620 rows x 768 columns]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flatten all channels to make pandas dataframe of images\n",
    "reshaped_flattened = images_flatten.reshape(images_flatten.shape[0], -1)\n",
    "print(reshaped_flattened.shape)\n",
    "df_img = pd.DataFrame(reshaped_flattened)\n",
    "df_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "ab28323e-5dd3-42d0-a56b-6fa15ca63b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2615</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2616</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2617</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2618</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2619</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2620 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1     2     3      4      5     6     7      8     9    ...  \\\n",
       "0     True   True  True  True   True   True  True  True   True  True  ...   \n",
       "1     True   True  True  True   True   True  True  True   True  True  ...   \n",
       "2     True   True  True  True   True  False  True  True   True  True  ...   \n",
       "3     True  False  True  True   True   True  True  True   True  True  ...   \n",
       "4     True   True  True  True   True   True  True  True   True  True  ...   \n",
       "...    ...    ...   ...   ...    ...    ...   ...   ...    ...   ...  ...   \n",
       "2615  True   True  True  True   True   True  True  True  False  True  ...   \n",
       "2616  True   True  True  True   True   True  True  True   True  True  ...   \n",
       "2617  True   True  True  True  False   True  True  True   True  True  ...   \n",
       "2618  True   True  True  True   True   True  True  True   True  True  ...   \n",
       "2619  True   True  True  True   True   True  True  True   True  True  ...   \n",
       "\n",
       "       758   759   760   761   762   763   764   765   766   767  \n",
       "0     True  True  True  True  True  True  True  True  True  True  \n",
       "1     True  True  True  True  True  True  True  True  True  True  \n",
       "2     True  True  True  True  True  True  True  True  True  True  \n",
       "3     True  True  True  True  True  True  True  True  True  True  \n",
       "4     True  True  True  True  True  True  True  True  True  True  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "2615  True  True  True  True  True  True  True  True  True  True  \n",
       "2616  True  True  True  True  True  True  True  True  True  True  \n",
       "2617  True  True  True  True  True  True  True  True  True  True  \n",
       "2618  True  True  True  True  True  True  True  True  True  True  \n",
       "2619  True  True  True  True  True  True  True  True  True  True  \n",
       "\n",
       "[2620 rows x 768 columns]"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use quartiles and IQR to detect outliers, withinLimit represents a dataframe where True if within limit and False if outlier\n",
    "Q1 = df_img.quantile(0.25)\n",
    "Q3 = df_img.quantile(0.75)\n",
    "InterquartileRange = Q3 - Q1\n",
    "lowerLim = Q1 - 1.5 * InterquartileRange\n",
    "upperLim = Q3 + 1.5 * InterquartileRange\n",
    "withinLimit = (df_img >= lowerLim) & (df_img <= upperLim)\n",
    "withinLimit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "930f7213-3499-45ea-911a-2917c4c89191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>253.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>43.031250</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>61.62500</td>\n",
       "      <td>23.515625</td>\n",
       "      <td>27.843750</td>\n",
       "      <td>20.09375</td>\n",
       "      <td>18.546875</td>\n",
       "      <td>21.656250</td>\n",
       "      <td>90.187500</td>\n",
       "      <td>50.15625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>237.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.031250</td>\n",
       "      <td>40.250000</td>\n",
       "      <td>36.84375</td>\n",
       "      <td>66.937500</td>\n",
       "      <td>82.750000</td>\n",
       "      <td>24.43750</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>27.546875</td>\n",
       "      <td>78.687500</td>\n",
       "      <td>33.43750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>237.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>...</td>\n",
       "      <td>47.375000</td>\n",
       "      <td>52.031250</td>\n",
       "      <td>92.31250</td>\n",
       "      <td>23.812500</td>\n",
       "      <td>72.812500</td>\n",
       "      <td>44.28125</td>\n",
       "      <td>55.750000</td>\n",
       "      <td>54.187500</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>83.31250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>...</td>\n",
       "      <td>94.812500</td>\n",
       "      <td>59.781250</td>\n",
       "      <td>35.90625</td>\n",
       "      <td>26.921875</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>76.50000</td>\n",
       "      <td>68.437500</td>\n",
       "      <td>46.125000</td>\n",
       "      <td>77.125000</td>\n",
       "      <td>61.93750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83.937500</td>\n",
       "      <td>19.796875</td>\n",
       "      <td>63.18750</td>\n",
       "      <td>86.125000</td>\n",
       "      <td>29.703125</td>\n",
       "      <td>48.62500</td>\n",
       "      <td>49.875000</td>\n",
       "      <td>65.375000</td>\n",
       "      <td>95.750000</td>\n",
       "      <td>34.37500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2615</th>\n",
       "      <td>145.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83.312500</td>\n",
       "      <td>24.750000</td>\n",
       "      <td>93.87500</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>42.125000</td>\n",
       "      <td>77.75000</td>\n",
       "      <td>20.406250</td>\n",
       "      <td>53.593750</td>\n",
       "      <td>51.406250</td>\n",
       "      <td>65.68750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2616</th>\n",
       "      <td>122.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26.296875</td>\n",
       "      <td>51.718750</td>\n",
       "      <td>50.78125</td>\n",
       "      <td>75.250000</td>\n",
       "      <td>62.562500</td>\n",
       "      <td>73.75000</td>\n",
       "      <td>80.250000</td>\n",
       "      <td>39.937500</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>51.09375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2617</th>\n",
       "      <td>44.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>...</td>\n",
       "      <td>86.437500</td>\n",
       "      <td>96.062500</td>\n",
       "      <td>76.50000</td>\n",
       "      <td>67.812500</td>\n",
       "      <td>34.375000</td>\n",
       "      <td>37.46875</td>\n",
       "      <td>66.937500</td>\n",
       "      <td>23.515625</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2618</th>\n",
       "      <td>129.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>56.375000</td>\n",
       "      <td>47.687500</td>\n",
       "      <td>31.56250</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>48.937500</td>\n",
       "      <td>23.81250</td>\n",
       "      <td>31.562500</td>\n",
       "      <td>75.562500</td>\n",
       "      <td>30.640625</td>\n",
       "      <td>32.81250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2619</th>\n",
       "      <td>64.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.093750</td>\n",
       "      <td>47.687500</td>\n",
       "      <td>60.40625</td>\n",
       "      <td>42.718750</td>\n",
       "      <td>49.250000</td>\n",
       "      <td>59.78125</td>\n",
       "      <td>69.062500</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>62.250000</td>\n",
       "      <td>87.37500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2620 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1      2      3      4      5      6      7      8      9    \\\n",
       "0     253.0  240.0  205.0   99.0    6.0   66.0   93.0    0.0   27.0    0.0   \n",
       "1     237.0  148.0    3.0   78.0  213.0  251.0  240.0  176.0  158.0  188.0   \n",
       "2     237.0  208.0  151.0   88.0   46.0    0.0  133.0  207.0  241.0  233.0   \n",
       "3      78.0    0.0  106.0  122.0  123.0   72.0   80.0  213.0  247.0  176.0   \n",
       "4       0.0   23.0  101.0  152.0  174.0  137.0   91.0   52.0   54.0   95.0   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2615  145.0  178.0  225.0  151.0   63.0   99.0  142.0  149.0    0.0  215.0   \n",
       "2616  122.0  119.0  121.0  163.0  210.0  225.0  211.0  157.0  130.0  131.0   \n",
       "2617   44.0   38.0   27.0    4.0    0.0   23.0   60.0   87.0  150.0  241.0   \n",
       "2618  129.0  161.0  217.0  226.0  198.0  112.0   51.0   57.0   45.0   17.0   \n",
       "2619   64.0  110.0  183.0  131.0   41.0    3.0    0.0   12.0   61.0  133.0   \n",
       "\n",
       "      ...        758        759       760        761        762       763  \\\n",
       "0     ...  43.031250  57.000000  61.62500  23.515625  27.843750  20.09375   \n",
       "1     ...  61.031250  40.250000  36.84375  66.937500  82.750000  24.43750   \n",
       "2     ...  47.375000  52.031250  92.31250  23.812500  72.812500  44.28125   \n",
       "3     ...  94.812500  59.781250  35.90625  26.921875  64.750000  76.50000   \n",
       "4     ...  83.937500  19.796875  63.18750  86.125000  29.703125  48.62500   \n",
       "...   ...        ...        ...       ...        ...        ...       ...   \n",
       "2615  ...  83.312500  24.750000  93.87500  88.000000  42.125000  77.75000   \n",
       "2616  ...  26.296875  51.718750  50.78125  75.250000  62.562500  73.75000   \n",
       "2617  ...  86.437500  96.062500  76.50000  67.812500  34.375000  37.46875   \n",
       "2618  ...  56.375000  47.687500  31.56250  30.015625  48.937500  23.81250   \n",
       "2619  ...  29.093750  47.687500  60.40625  42.718750  49.250000  59.78125   \n",
       "\n",
       "            764        765        766       767  \n",
       "0     18.546875  21.656250  90.187500  50.15625  \n",
       "1     57.000000  27.546875  78.687500  33.43750  \n",
       "2     55.750000  54.187500  30.328125  83.31250  \n",
       "3     68.437500  46.125000  77.125000  61.93750  \n",
       "4     49.875000  65.375000  95.750000  34.37500  \n",
       "...         ...        ...        ...       ...  \n",
       "2615  20.406250  53.593750  51.406250  65.68750  \n",
       "2616  80.250000  39.937500  66.000000  51.09375  \n",
       "2617  66.937500  23.515625  64.750000  64.75000  \n",
       "2618  31.562500  75.562500  30.640625  32.81250  \n",
       "2619  69.062500  72.500000  62.250000  87.37500  \n",
       "\n",
       "[2620 rows x 768 columns]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace outliers with 0\n",
    "median = df_img.median(axis = 0)\n",
    "df_img = df_img.where(withinLimit, 0, axis = 0)\n",
    "df_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "101e2956-6f73-4a7e-b539-69e06a4e76fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2620, 3, 256)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2620, 3, 256)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reconstruct image_flatten using dataframe\n",
    "reconstructed = df_img.to_numpy().reshape(n, c, h*w)\n",
    "print(images_flatten.shape)\n",
    "reconstructed.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4916043",
   "metadata": {},
   "source": [
    "### 4. Detection and Handling of Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "ad3ab20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1196\n"
     ]
    }
   ],
   "source": [
    "zeroClass = df.value_counts()[0]\n",
    "oneClass = df.value_counts()[1]\n",
    "twoClass = df.value_counts()[2]\n",
    "\n",
    "#2:1:1 ratio is acceptable where imbalanced data shouldn't be a problem.\n",
    "n = (int) (zeroClass / 2)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "eeda5c02-a349-4c18-be60-ff4e7c49ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversample 1 and 2 s.t. 2 : 1 : 1 ratio is achieved\n",
    "zeros = df[df[0] == 0]\n",
    "ones = df[df[0] == 1]\n",
    "twos = df[df[0] == 2]\n",
    "ones = ones.sample(n=(int)(zeroClass/2), replace=True)\n",
    "twos = twos.sample(n=(int)(zeroClass/2), replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "f218f442-4b5a-4ac9-8ed9-8d369b94824e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    2392\n",
       "1.0    1196\n",
       "2.0    1196\n",
       "dtype: int64"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([zeros, ones, twos], axis = 0)\n",
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "d414fbec-377a-42b6-8476-720fe0a1049e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([   1,    5,    7,    9,   10,   11,   12,   13,   14,   15,\n",
      "            ...\n",
      "            2121, 1600,   31, 2458,   46, 1159,  749, 1706,  636, 2071],\n",
      "           dtype='int64', length=4784)\n",
      "(4784, 3, 16, 16)\n",
      "(4784, 1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2552a795",
   "metadata": {},
   "source": [
    "### 5. Understanding Relationship Between Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757fb315",
   "metadata": {},
   "source": [
    "### 6. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "93f82e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "1.0\n",
      "0.0\n",
      "2.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "images_transposed = np.transpose(images[0], (1, 2, 0)) / 255\n",
    "imagetoshow = images_transposed.astype('float32')\n",
    "plt.imshow(imagetoshow)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 16))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = np.transpose(images[i], (1, 2, 0)) / 255\n",
    "    img = img.astype('float32')\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')  # Turn off axis numbers\n",
    "    print(labels[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7eebcf",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e3383",
   "metadata": {},
   "source": [
    "### 7. General Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "8e35eeb1-8af1-4282-a252-0482922411b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[253.  , 240.  , 205.  , ..., 166.  , 153.  , 146.  ],\n",
       "        [202.  , 126.  ,  23.  , ...,  15.  , 236.  , 107.  ],\n",
       "        [ 79.6 ,  56.06,  24.12, ...,  21.66,  90.2 ,  50.16]],\n",
       "\n",
       "       [[237.  , 148.  ,   3.  , ...,  86.  ,  27.  ,   0.  ],\n",
       "        [126.  , 144.  , 161.  , ...,  34.  , 199.  ,  53.  ],\n",
       "        [ 56.06,  61.62,  66.94, ...,  27.55,  78.7 ,  33.44]],\n",
       "\n",
       "       [[237.  , 208.  , 151.  , ..., 210.  , 138.  ,  93.  ],\n",
       "        [ 97.  ,  12.  ,  46.  , ..., 120.  ,  43.  , 214.  ],\n",
       "        [ 47.06,  20.72,  31.27, ...,  54.2 ,  30.33,  83.3 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 44.  ,  38.  ,  27.  , ...,  71.  ,  32.  ,   8.  ],\n",
       "        [ 95.  , 168.  , 108.  , ...,  21.  , 154.  , 154.  ],\n",
       "        [ 46.44,  69.06,  50.47, ...,  23.52,  64.75,  64.75]],\n",
       "\n",
       "       [[129.  , 161.  , 217.  , ..., 227.  , 212.  , 201.  ],\n",
       "        [216.  , 123.  ,  31.  , ..., 189.  ,  44.  ,  51.  ],\n",
       "        [ 83.94,  55.12,  26.61, ...,  75.56,  30.64,  32.8 ]],\n",
       "\n",
       "       [[ 64.  , 110.  , 183.  , ...,  44.  ,  77.  ,  98.  ],\n",
       "        [ 72.  , 112.  ,  49.  , ..., 179.  , 146.  , 227.  ],\n",
       "        [ 39.3 ,  51.72,  32.2 , ...,  72.5 ,  62.25,  87.4 ]]],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_flatten = np.clip(images_flatten, 0, 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3aa527",
   "metadata": {},
   "source": [
    "### 8. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921e8ca",
   "metadata": {},
   "source": [
    "### 9. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa676c3f",
   "metadata": {},
   "source": [
    "## Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b37e4",
   "metadata": {},
   "source": [
    "### 10. Creating models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495bf3c0",
   "metadata": {},
   "source": [
    "### 10 + 11. Model Creation and Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6595e4-a571-4d9f-a87b-82fd55550085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Neural Network with Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "class ModelCNNAdam(nn.Module):  \n",
    "    \"\"\"\n",
    "    This class represents an AI model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "        self.l1 = nn.Linear(256, 1024)\n",
    "        self.l2 = nn.Linear(1024, 3)\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "        self.dropout = nn.Dropout(p = 0.3)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.norm1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.norm2(self.conv2(x))))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.relu(self.l1(x))\n",
    "        x = self.dropout(self.relu(self.l2(x)))\n",
    "        return x\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using the input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Training data.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of the trained model.\n",
    "            \n",
    "        \"\"\"\n",
    "        n, c, h, w = X.shape\n",
    "        #set NaNs to 0 in image data and clip values to 0,255 to avoid outliers\n",
    "        images = np.clip(np.nan_to_num(X), 0, 255)\n",
    "        df = pd.DataFrame(y)\n",
    "        zeroClass = df.value_counts()[0]\n",
    "        oneClass = df.value_counts()[1]\n",
    "        twoClass = df.value_counts()[2]\n",
    "        zeros = df[df[0] == 0]\n",
    "        ones = df[df[0] == 1]\n",
    "        twos = df[df[0] == 2]\n",
    "        ones = ones.sample(n=(int)(zeroClass), replace=True)\n",
    "        twos = twos.sample(n=(int)(zeroClass), replace=True)\n",
    "        df = pd.concat([zeros, ones, twos], axis = 0)\n",
    "        #Making final X and Y\n",
    "        finalY = df.to_numpy()\n",
    "        indicesY = df.index\n",
    "        finalX = images[indicesY]\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = 0.001)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        Xtrain = torch.tensor(finalX, dtype=torch.float32)\n",
    "        Ytrain = torch.tensor(finalY, dtype=torch.long).squeeze()\n",
    "        #Training loop\n",
    "        for epoch in range(50):\n",
    "            optimizer.zero_grad()\n",
    "            output = self(Xtrain)\n",
    "            loss = loss_fn(output, Ytrain)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (epoch % 10 == 0):\n",
    "                print(f'Loss: {loss.item()}')\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained model to make predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "        Predicted target values per element in X.\n",
    "           \n",
    "        \"\"\"\n",
    "        #set nan pixels to 0 and clip to 0, 255\n",
    "        X = np.clip(np.nan_to_num(X), 0,255)\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self(X)\n",
    "        return torch.argmax(output, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39056761-8022-4d59-a6bc-ee7eeff77b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Neural Network with SGD (not used)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "class ModelCNNSGD(nn.Module):  \n",
    "    \"\"\"\n",
    "    This class represents an AI model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ModelCNNSGD, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "        self.l1 = nn.Linear(256, 1024)\n",
    "        self.l2 = nn.Linear(1024, 3)\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "        self.dropout = nn.Dropout(p = 0.3)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.norm1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.norm2(self.conv2(x))))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.relu(self.l1(x))\n",
    "        x = self.dropout(self.relu(self.l2(x)))\n",
    "        return x\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using the input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Training data.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of the trained model.\n",
    "            \n",
    "        \"\"\"\n",
    "        n, c, h, w = X.shape\n",
    "        #set NaNs to 0 in image data and clip values to 0,255 to avoid outliers\n",
    "        images = np.clip(np.nan_to_num(X), 0, 255)\n",
    "        df = pd.DataFrame(y)\n",
    "        zeroClass = df.value_counts()[0]\n",
    "        oneClass = df.value_counts()[1]\n",
    "        twoClass = df.value_counts()[2]\n",
    "        zeros = df[df[0] == 0]\n",
    "        ones = df[df[0] == 1]\n",
    "        twos = df[df[0] == 2]\n",
    "        ones = ones.sample(n=(int)(zeroClass), replace=True)\n",
    "        twos = twos.sample(n=(int)(zeroClass), replace=True)\n",
    "        df = pd.concat([zeros, ones, twos], axis = 0)\n",
    "        #Making final X and Y\n",
    "        finalY = df.to_numpy()\n",
    "        indicesY = df.index\n",
    "        finalX = images[indicesY]\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr = 0.001)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        Xtrain = torch.tensor(finalX, dtype=torch.float32)\n",
    "        Ytrain = torch.tensor(finalY, dtype=torch.long).squeeze()\n",
    "        #Training loop\n",
    "        for epoch in range(50):\n",
    "            optimizer.zero_grad()\n",
    "            output = self(Xtrain)\n",
    "            loss = loss_fn(output, Ytrain)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (epoch % 10 == 0):\n",
    "                print(f'Loss: {loss.item()}')\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained model to make predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "        Predicted target values per element in X.\n",
    "           \n",
    "        \"\"\"\n",
    "        #set nan pixels to 0 and clip to 0, 255\n",
    "        X = np.clip(np.nan_to_num(X), 0,255)\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self(X)\n",
    "        return torch.argmax(output, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5dedd0b-6bba-4755-9109-b77287e6838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA MLP NN (not used)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA\n",
    "class ModelPCA(nn.Module):  \n",
    "    \"\"\"\n",
    "    This class represents an AI model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ModelPCA, self).__init__()\n",
    "        self.pca = PCA(n_components = 512)\n",
    "        self.l1 = nn.Linear(512, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(512, 128)\n",
    "        self.l3 = nn.Linear(128, 3)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.relu(self.l1(x)))\n",
    "        x = self.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using the input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Training data.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of the trained model.\n",
    "            \n",
    "        \"\"\"\n",
    "        n, c, h, w = X.shape\n",
    "        df = pd.DataFrame(y)\n",
    "        flattened = X.reshape(n, -1)\n",
    "        flattened = np.nan_to_num(flattened)\n",
    "        flattened = np.clip(flattened, 0, 255)\n",
    "        #PCA Dimension reduction\n",
    "        images_reduced = self.pca.fit_transform(flattened)\n",
    "        #oversampling\n",
    "        zeroClass = df.value_counts()[0]\n",
    "        oneClass = df.value_counts()[1]\n",
    "        twoClass = df.value_counts()[2]\n",
    "        zeros = df[df[0] == 0]\n",
    "        ones = df[df[0] == 1]\n",
    "        twos = df[df[0] == 2]\n",
    "        ones = ones.sample(n=(int)(zeroClass), replace=True)\n",
    "        twos = twos.sample(n=(int)(zeroClass), replace=True)\n",
    "        df = pd.concat([zeros, ones, twos], axis = 0)\n",
    "\n",
    "        #Making final X and Y\n",
    "        finalY = df.to_numpy()\n",
    "        indicesY = df.index\n",
    "        finalX = images_reduced[indicesY]\n",
    "\n",
    "        #training\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = 0.001)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        Xtrain = torch.tensor(finalX, dtype=torch.float32)\n",
    "        Ytrain = torch.tensor(finalY, dtype=torch.long).squeeze()\n",
    "        for epoch in range(50):\n",
    "            optimizer.zero_grad()\n",
    "            output = self(Xtrain)\n",
    "            loss = loss_fn(output, Ytrain)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (epoch % 10 == 0):\n",
    "                print(f'Loss: {loss.item()}')\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained model to make predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "        Predicted target values per element in X.\n",
    "           \n",
    "        \"\"\"\n",
    "        print(X.shape)\n",
    "        X = np.clip(np.nan_to_num(X), 0, 255).reshape(X.shape[0], -1)\n",
    "        print(X.shape)\n",
    "        X_reduced = self.pca.transform(X)\n",
    "        print(X_reduced.shape)\n",
    "        X = torch.tensor(X_reduced, dtype=torch.float32)\n",
    "        self.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self(X)\n",
    "        return torch.argmax(output, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18a0604e-e3b2-4feb-9868-608de05342b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1232ea0-6169-4b70-9e80-f8e925baea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open('data.npy', 'rb') as f:\n",
    "    data = np.load(f, allow_pickle=True).item()\n",
    "    X = data['image']\n",
    "    y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c92be07-fedb-43a2-9124-0b8fff0c4341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.082059860229492\n",
      "Loss: 0.48684126138687134\n",
      "Loss: 0.08093351870775223\n",
      "Loss: 0.0020691384561359882\n",
      "Loss: 0.0006119367317296565\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m modelPCA \u001b[38;5;241m=\u001b[39m ModelPCA()\n\u001b[0;32m     13\u001b[0m modelPCA\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m---> 14\u001b[0m y_predPCA \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     16\u001b[0m modelCNNAdam \u001b[38;5;241m=\u001b[39m ModelPCA()\n\u001b[0;32m     17\u001b[0m modelCNNAdam\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "# Filter test data that contains no labels\n",
    "# In Coursemology, the test data is guaranteed to have labels\n",
    "nan_indices = np.argwhere(np.isnan(y_test)).squeeze()\n",
    "mask = np.ones(y_test.shape, bool)\n",
    "mask[nan_indices] = False\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "# Train and predict\n",
    "modelPCA = ModelPCA()\n",
    "modelPCA.fit(X_train, y_train)\n",
    "y_predPCA = model.predict(X_test)\n",
    "\n",
    "modelCNNAdam = ModelPCA()\n",
    "modelCNNAdam.fit(X_train, y_train)\n",
    "y_predCNNAdam = model.predict(X_test)\n",
    "\n",
    "modelCNNSGD = ModelPCA()\n",
    "modelCNNSGD.fit(X_train, y_train)\n",
    "y_predCNNSGD = model.predict(X_test)\n",
    "\n",
    "# Evaluate model predition\n",
    "# Learn more: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "print(\"F1 Score (PCA): {0:.2f}\".format(f1_score(y_test, y_predPCA, average='macro')))\n",
    "print(\"F1 Score (CNN Adam): {0:.2f}\".format(f1_score(y_test, y_predCNNAdam, average='macro')))\n",
    "print(\"F1 Score (CNN SGD): {0:.2f}\".format(f1_score(y_test, y_predCNNSGD, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0562c7c2-0981-4681-8ea7-6d299666e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bonus. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "b07e0a43-ea9f-4ad4-951a-573da05f9281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "class TestModel(nn.Module):  \n",
    "    \"\"\"\n",
    "    This class represents an AI model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TestModel, self).__init__()\n",
    "        self.lr = 1e-3\n",
    "        self.conv1 = nn.Conv2d(3, 32, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "        self.l1 = nn.Linear(256, 256)\n",
    "        self.l2 = nn.Linear(256, 3)\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "        self.dropout = nn.Dropout(p = 0.1)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.norm1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.norm2(self.conv2(x))))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.relu(self.l1(x))\n",
    "        x = self.dropout(self.relu(self.l2(x)))\n",
    "        return x\n",
    "\n",
    "    def init2(self, lr, dr, nne):\n",
    "        self.lr = lr\n",
    "        self.dropout = nn.Dropout(p = dr)\n",
    "        self.l1 = nn.Linear(256, nne)\n",
    "        self.l2 = nn.Linear(nne, 3)\n",
    "        return self\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using the input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Training data.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of the trained model.\n",
    "            \n",
    "        \"\"\"\n",
    "        n, c, h, w = X.shape\n",
    "        #set NaNs to 0 in image data and clip values to 0,255 to avoid outliers\n",
    "        images = np.clip(np.nan_to_num(X), 0, 255)\n",
    "        df = pd.DataFrame(y)\n",
    "        zeroClass = df.value_counts()[0]\n",
    "        oneClass = df.value_counts()[1]\n",
    "        twoClass = df.value_counts()[2]\n",
    "\n",
    "        zeros = df[df[0] == 0]\n",
    "        ones = df[df[0] == 1]\n",
    "        twos = df[df[0] == 2]\n",
    "        ones = ones.sample(n=(int)(zeroClass), replace=True)\n",
    "        twos = twos.sample(n=(int)(zeroClass), replace=True)\n",
    "        df = pd.concat([zeros, ones, twos], axis = 0)\n",
    "\n",
    "        #Making final X and Y\n",
    "        finalY = df.to_numpy()\n",
    "        indicesY = df.index\n",
    "        finalX = images[indicesY]\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = self.lr)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        Xtrain = torch.tensor(finalX, dtype=torch.float32)\n",
    "        Ytrain = torch.tensor(finalY, dtype=torch.long).squeeze()\n",
    "        #Training loop\n",
    "        for epoch in range(50):\n",
    "            optimizer.zero_grad()\n",
    "            output = self(Xtrain)\n",
    "            loss = loss_fn(output, Ytrain)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #if (epoch % 10 == 0):\n",
    "                #print(f'Loss: {loss.item()}')\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained model to make predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "        Predicted target values per element in X.\n",
    "           \n",
    "        \"\"\"\n",
    "        #set nan pixels to 0 and clip to 0, 255\n",
    "        X = np.clip(np.nan_to_num(X), 0,255)\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self(X)\n",
    "        return torch.argmax(output, axis = 1)\n",
    "        #print(torch.argmax(output, dim=1))\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "c591a334-7162-4ccf-a344-94906f7aa1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing learning rate:  0.001\n",
      "iteration  0\n",
      "iteration  1\n",
      "iteration  2\n",
      "[0.5266760431317393, 0.527396531545909, 0.5159552845528456]\n",
      "testing learning rate:  0.005\n",
      "iteration  0\n",
      "iteration  1\n",
      "iteration  2\n",
      "[0.5039736824962543, 0.5162974280621339, 0.5174238129943045]\n",
      "testing learning rate:  0.01\n",
      "iteration  0\n",
      "iteration  1\n",
      "iteration  2\n",
      "[0.3807051282051282, 0.4944919903445249, 0.43748511550369135]\n"
     ]
    }
   ],
   "source": [
    "#non-exhaustive grid search for hyperparameters\n",
    "\n",
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "# Filter test data that contains no labels\n",
    "# In Coursemology, the test data is guaranteed to have labels\n",
    "nan_indices = np.argwhere(np.isnan(y_test)).squeeze()\n",
    "mask = np.ones(y_test.shape, bool)\n",
    "mask[nan_indices] = False\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "#initialise hyperparameters\n",
    "learning_rates = [1e-3, 5e-3, 1e-2]\n",
    "dropout_rates = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "num_neurons_l1 = [256, 512, 1024]\n",
    "best_hyperparams = [1e-3, 0.2, 512, 0.0]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for dr in dropout_rates:\n",
    "        for nne in num_neurons_l1:\n",
    "            model = TestModel().init2(lr, dr, nne)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            score = f1_score(y_test, y_pred, average = 'macro')\n",
    "            if score > best_hyperparams[3]:\n",
    "                best_hyperparams[3] = score\n",
    "                best_hyperparams[0] = lr\n",
    "                best_hyperparams[1] = dr\n",
    "                best_hyperparams[2] = nne\n",
    "                print('new highscore: ', score)\n",
    "                print(lr, dr, nne)\n",
    "            else:\n",
    "                print(score)\n",
    "\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81addd51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
