def train_model(loader, model):
    """
    PARAMS
    loader: the data loader used to generate training batches
    model: the model to train
  
    RETURNS
        the final trained model 
    """
    """
    YOUR CODE HERE
    
    - create the loss and optimizer
    """
    optimiser = torch.optim.Adam(model.parameters())
    loss_fn = nn.CrossEntropyLoss()
    epoch_losses = []
    for i in range(10):
        epoch_loss = 0
        
        for idx, data in enumerate(loader):
            x, y = data
            """
            YOUR CODE HERE
            
            - reset the optimizer
            - perform forward pass
            - compute loss
            - perform backward pass
            """
            optimiser.zero_grad()
            y_pred = model(x)
            loss = loss_fn(y_pred, y)
            loss.backward()
            optimiser.step()
            # COMPUTE STATS
            epoch_loss += loss.item()
        epoch_loss = epoch_loss / len(loader)
        epoch_losses.append(epoch_loss)
        print ("Epoch: {}, Loss: {}".format(i, epoch_loss))
        
    # YOUR CODE HERE
    return (model, epoch_losses)
