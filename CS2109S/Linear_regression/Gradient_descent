def gradient_descent_one_variable(x, y, lr = 1e-5, number_of_epochs = 250):
    '''
    Approximate bias and weight that gave the best fitting line.
    Parameters
    ----------
    x (np.ndarray) : (m, 1) numpy matrix representing a feature column
    y (np.ndarray) : (m, 1) numpy matrix representing target values
    lr (float) : Learning rate
    number_of_epochs (int) : Number of gradient descent epochs
    
    Returns
    -------
        bias (float):
            The bias constant
        weight (float):
            The weight constant
        loss (list):
            A list where the i-th element denotes the MSE score at i-th epoch.
    '''
    # Do not change
    bias = 0
    weight = 0
    loss = []
    
    m = y.shape[0]
    #todo
    y_pred = (x * weight) + bias
    for epochs in range(number_of_epochs):
        biasDerive = (1/m) * np.sum(y_pred - y)
        weightDerive = (1/m) * np.sum((y_pred - y) * x)
        bias -= (lr * biasDerive)
        weight -= (lr * weightDerive)
        y_pred = (x * weight) + bias
        loss.append(mean_squared_error(y, y_pred))
    return bias, weight, loss



def gradient_descent_multi_variable(X, y, lr = 1e-5, number_of_epochs = 250):
    '''
    Approximate bias and weight that gave the best fitting line.
    Parameters
    ----------
    X (np.ndarray) : (m, n) numpy matrix representing feature matrix
    y (np.ndarray) : (m, 1) numpy matrix representing target values
    lr (float) : Learning rate
    number_of_epochs (int) : Number of gradient descent epochs
    
    Returns
    -------
        bias (float):
            The bias constant
        weights (np.ndarray):
            A (n, 1) numpy matrix that specifies the weight constants.
        loss (list):
            A list where the i-th element denotes the MSE score at i-th epoch.
    '''
    # Do not change
    bias = 0
    weights = np.full((X.shape[1], 1), 0).astype(float)
    loss = []
    m = y.shape[0]
    for _ in range(number_of_epochs):
        y_pred = np.matmul(X, weights) + bias
        error = (y_pred - y)
        # TODO: add your solution here and remove `raise NotImplementedError`
        biasDerive = (1/m) * np.sum(error)
        weightsDerive = (1/m) * np.matmul(np.transpose(X), error)
        bias -= (lr * biasDerive)
        weights -= (lr * weightsDerive)
        y_pred = np.matmul(X, weights) + bias
        loss.append(mean_squared_error(y, y_pred))
    
    return bias, weights, loss
